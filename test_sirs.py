import os
import time
import torch
import pandas as pd  # å»ºè­°å®‰è£ä»¥æ–¹ä¾¿è™•ç†è¡¨æ ¼
from os.path import join
import torch.backends.cudnn as cudnn
from ptflops import get_model_complexity_info

import data.sirs_dataset as datasets
from engine import Engine
from options.net_options.train_options import TrainOptions
from tools import mutils
import util.util as util

# --- ç’°å¢ƒèˆ‡åƒæ•¸è¨­å®š ---
opt = TrainOptions().parse()
opt.isTrain = False
cudnn.benchmark = True
opt.no_log = True
opt.display_id = 0
opt.verbose = False

# å¼·åˆ¶æŒ‡å®šæ‚¨çš„æ¬Šé‡è·¯å¾‘
opt.weight_path = "/content/drive/MyDrive/Colab Notebooks/Term_Project/DSRNet/checkpoints/dsrnet_l/dsrnet_l_DSRNet_3000.pth"

# åˆå§‹åŒ– Engine
engine = Engine(opt)

# --- æ•ˆèƒ½æ•¸æ“š (åœ–äºŒ): Parameters & FLOPs ---
print("\n" + "=" * 50)
print("ğŸ“Š æ­£åœ¨è¨ˆç®—æ¨¡å‹æ•ˆèƒ½æ•¸æ“š (Table 2)...")
with torch.cuda.device(0):
    macs, params = get_model_complexity_info(
        engine.model.netG, (3, 224, 224), as_strings=True, print_per_layer_stat=False
    )
    perf_msg = f"Parameters: {params} | FLOPs: {macs}"
    print(perf_msg)
print("=" * 50 + "\n")

# --- å®šç¾© 7 å€‹æ¸¬è©¦è³‡æ–™é›†è·¯å¾‘ ---
# é€™äº›è·¯å¾‘åš´æ ¼å°æ‡‰æ‚¨çš„é›²ç«¯è³‡æ–™å¤¾çµæ§‹
test_base = "/content/drive/MyDrive/Colab Notebooks/Term_Project/testing set"

test_configs = {
    "Berkeley real20_420": join(test_base, "Berkeley real20_420"),
    "CEILNet_real45": join(test_base, "CEILNet_real45"),
    "NRD": join(test_base, "Natural Reflection Dataset(NRD)"),
    "Nature": join(test_base, "Nature"),
    "SIR2_Postcard": join(test_base, "SIR2/PostcardDataset"),
    "SIR2_SolidObject": join(test_base, "SIR2/SolidObjectDataset"),
    "SIR2_WildScene": join(test_base, "SIR2/WildSceneDataset"),
}

# --- çµæœè¼¸å‡ºè¨­å®š ---
result_save_path = "/content/drive/MyDrive/Colab Notebooks/Term_Project/DSRNet/checkpoints/dsrnet_l/DSRNet_result.txt"
report_file = open(result_save_path, "w", encoding="utf-8")
report_file.write(f"DSRNet Test Report - {mutils.get_formatted_time()}\n")
report_file.write(f"Model Weight: {opt.weight_path}\n")
report_file.write(f"Model Info: {perf_msg}\n")
report_file.write("=" * 80 + "\n")
report_file.write(
    f"{'Dataset':<20} | {'PSNR':<8} | {'SSIM':<8} | {'NCC':<8} | {'LMSE':<8} | {'LPIPS':<8} | {'Time':<8}\n"
)
report_file.write("-" * 80 + "\n")

"""Main Testing Loop"""
for label, path in test_configs.items():
    if not os.path.exists(path):
        print(f"âš ï¸ è·³é {label}: æ‰¾ä¸åˆ°è·¯å¾‘ {path}")
        continue

    print(f"\nğŸš€ æ­£åœ¨è©•ä¼°è³‡æ–™é›†: {label}")

    # æ ¹æ“šè³‡æ–™é›†ç‰¹æ€§è¼‰å…¥ (NRD çš„è³‡æ–™å¤¾çµæ§‹èˆ‡å…¶ä»–ä¸åŒï¼Œéœ€ç‰¹åˆ¥è™•ç†)
    # NRD ä½¿ç”¨ NCCU_I (æ¸¬è©¦) èˆ‡ NCCU_T (é©—è­‰)
    if "NRD" in label:
        eval_dataset = datasets.DSRTestDataset(path, if_align=opt.if_align)
        # è¨»ï¼šéœ€ç¢ºä¿ datasets.DSRTestDataset å…§éƒ¨æœ‰å°æ‡‰ NCCU_I/T çš„é‚è¼¯
    else:
        eval_dataset = datasets.DSRTestDataset(path, if_align=opt.if_align)

    eval_dataloader = datasets.DataLoader(
        eval_dataset,
        batch_size=1,
        shuffle=False,
        num_workers=opt.nThreads,
        pin_memory=True,
    )

    # åŸ·è¡Œè©•ä¼°
    # engine.eval å…§éƒ¨æœƒå‘¼å«ä¹‹å‰åœ¨ util.py æ”¹å¥½çš„ batch_all_metrics
    start_time = time.time()
    avg_meters = engine.eval(eval_dataloader, dataset_name=label)
    end_time = time.time()

    avg_run_time = (end_time - start_time) / len(eval_dataloader)

    # å¯«å…¥æ–‡å­—æª”å ±å‘Š
    line = f"{label:<20} | {avg_meters['PSNR']:<8.4f} | {avg_meters['SSIM']:<8.4f} | {avg_meters['NCC']:<8.4f} | {avg_meters['LMSE']:<8.4f} | {avg_meters['LPIPS']:<8.4f} | {avg_run_time:<8.4f}s\n"
    report_file.write(line)
    report_file.flush()  # å³æ™‚å­˜æª”é˜²æ­¢æ–·ç·š

    print(f"âœ… {label} æŒ‡æ¨™å·²å­˜å…¥å ±å‘Š")

report_file.write("=" * 80 + "\n")
report_file.close()

print(f"\nâœ¨ æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼çµæœå ±å‘Šå·²è¼¸å‡ºè‡³: {result_save_path}")
